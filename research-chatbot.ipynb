{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import aws_region_name, aws_access_key, aws_secret_key\n",
    "import boto3\n",
    "def create_client():\n",
    "    '''\n",
    "    Create AWS Bedrock client to interact with LLM\n",
    "    '''\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime',\n",
    "                        region_name=aws_region_name,\n",
    "                        aws_access_key_id=aws_access_key,\n",
    "                        aws_secret_access_key=aws_secret_key\n",
    "                        )\n",
    "    return bedrock\n",
    "bedrock_client = create_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "def load_pdfs(chunk_size=3000, chunk_overlap=100):\n",
    "\n",
    "    # load the pdf documents\n",
    "    loader=PyPDFDirectoryLoader(\"PDF Documents\")\n",
    "    documents=loader.load()\n",
    "\n",
    "    # split the documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                   chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents=documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "def create_vector_store(docs):\n",
    "\n",
    "    # Set up bedrock client\n",
    "    bedrock = create_client()\n",
    "    bedrock_embeddings=BedrockEmbeddings(model_id='amazon.titan-embed-text-v1', client=bedrock)\n",
    "\n",
    "    # create and save the vector store\n",
    "    vector_store = FAISS.from_documents(docs, bedrock_embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "def create_llm(bedrock_client, model_id, model_kwargs):\n",
    "\n",
    "    # load llama2 \n",
    "    llm = Bedrock(model_id='meta.llama2-13b-chat-v1', \n",
    "                  client=bedrock_client,\n",
    "                  #streaming=True,\n",
    "                  model_kwargs=model_kwargs)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'temperature':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 13b chat\n",
    "llm = create_llm(bedrock_client, 'meta.llama2-13b-chat-v1', model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 70b instruct\n",
    "llm = create_llm(bedrock_client, 'meta.llama3-70b-instruct-v1:0', model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retrieval chain\n",
    "\n",
    "It will receive user query -> retrieve most similar chunks from vector store -> send embedded query and retrieved chunks to loaded LLM, generating response with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "# load llm\n",
    "llm = create_llm(bedrock_client=bedrock_client)\n",
    "\n",
    "# load embeddings and vector store\n",
    "bedrock_embeddings=BedrockEmbeddings(model_id='amazon.titan-embed-text-v1', client=bedrock_client)\n",
    "vector_store = FAISS.load_local('faiss_index', bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# create memory history\n",
    "message_history = ChatMessageHistory()\n",
    "# store past queries for follow up questions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key=\"answer\",\n",
    "    chat_memory=message_history,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# create qa chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, \n",
    "                                       chain_type='stuff', \n",
    "                                       retriever=vector_store.as_retriever(search_type='similarity', search_kwargs={\"k\":3}), # use 3 embeddings from context\n",
    "                                       return_source_documents=True,\n",
    "                                       memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build UI with Chainlit\n",
    "\n",
    "Chainlit is used for creating the function that establishes the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def create_qa_chain():\n",
    "\n",
    "    # create client \n",
    "    bedrock_client = create_client()\n",
    "\n",
    "    # load llm\n",
    "    llm = create_llm(bedrock_client=bedrock_client)\n",
    "\n",
    "    # load embeddings and vector store\n",
    "    bedrock_embeddings=BedrockEmbeddings(model_id='amazon.titan-embed-text-v1', client=bedrock_client)\n",
    "    vector_store = FAISS.load_local('faiss_index', bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # create memory history\n",
    "    message_history = ChatMessageHistory()\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        chat_memory=message_history,\n",
    "        return_messages=True,\n",
    "    )\n",
    "\n",
    "    # create qa chain\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(llm, \n",
    "                                           chain_type='stuff', \n",
    "                                           retriever=vector_store.as_retriever(search_type='similarity', search_kwargs={\"k\":3}),\n",
    "                                           return_source_documents=True,\n",
    "                                           memory=memory\n",
    "                                           )\n",
    "    \n",
    "    # add custom messages to the user interface\n",
    "    msg = cl.Message(content=\"Loading the bot...\")\n",
    "    await msg.send()\n",
    "    msg.content = \"Hi, Welcome to the QA Chatbot! Please ask your question.\"\n",
    "    await msg.update()\n",
    "    \n",
    "    cl.user_session.set('qa_chain' ,qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@cl.on_message\n",
    "async def generate_response(query):\n",
    "    qa_chain = cl.user_session.get('qa_chain')\n",
    "\n",
    "    res = await qa_chain.acall(query.content, callbacks=[cl.AsyncLangchainCallbackHandler(\n",
    "        stream_final_answer=True, \n",
    "        )])\n",
    "\n",
    "    # extract results and source documents\n",
    "    result, source_documents = res['answer'], res['source_documents']\n",
    "\n",
    "    # Extract all values associated with the 'metadata' key\n",
    "    source_documents = str(source_documents)\n",
    "    metadata_values = re.findall(r\"metadata={'source': '([^']*)', 'page': (\\d+)}\", source_documents)\n",
    "\n",
    "    # Convert metadata_values into a single string\n",
    "    pattern = r'PDF Documents|\\\\'\n",
    "    metadata_string = \"\\n\".join([f\"Source: {re.sub(pattern, '', source)}, page: {page}\" for source, page in metadata_values])\n",
    "\n",
    "    # add metadata (i.e., sources) to the results\n",
    "    result += f'\\n\\n{metadata_string}'\n",
    "\n",
    "    # send the generated response to the user\n",
    "    await cl.Message(content=result).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing CrewAI\n",
    "\n",
    "- Agents: These are standalone units programmed to perform tasks , make decisions and communicate with other agents. They can use Tools which can be simple search functions or complex integrations involving other chains, APIs, etc.\n",
    "- Tasks: Tasks are assignments or jobs that an AI agent needs to complete. They can include additional information like which agent should do it and what tools they might need.\n",
    "- Crews: A Crew is a team of agents, each with a specific role, that work together to achieve a common goal. The process of forming a crew involves assembling agents, defining their tasks and establishing a sequence of task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"max_gen_len\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "llm = create_llm(bedrock_client, 'meta.llama3-70b-instruct-v1:0', model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Meta learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pdf_links(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pdf_links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        #print(link)\n",
    "        href = a_tag.get('href')\n",
    "        content = a_tag.text.strip()\n",
    "        #print(\"a tag content is\", content)\n",
    "\n",
    "        # If the link ends with \".pdf\" or \"PDF\" is in the text\n",
    "        if href:\n",
    "            pdf_in_children = any('PDF' in child.text.upper() for child in a_tag.find_all())\n",
    "            # for child in a_tag.find_all():\n",
    "            #     print(child)\n",
    "            #     if \"PDF\" in child.text.upper():\n",
    "            #         print(child.text)\n",
    "            if href.endswith('.pdf') or 'PDF' in content.upper() or pdf_in_children:\n",
    "                pdf_links.append(href)\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will perform a Google Scholar search, grab the top_n results, and extract the PDF links from those results\n",
    "# Note: Some websites require purchasing the PDF or logging in. We will not get links back for these cases\n",
    "def google_scholar_search(query, top_n=5):\n",
    "    search_query = scholarly.search_pubs(query)\n",
    "    pdf_links = []\n",
    "    for i in range(top_n):\n",
    "        try:\n",
    "            result = next(search_query)\n",
    "            url = result['pub_url']\n",
    "            #print(url)\n",
    "\n",
    "            pdf_link = ''\n",
    "            if url.endswith(\".pdf\"):\n",
    "                pdf_link = url\n",
    "            else:\n",
    "                scraped_pdf_links = scrape_pdf_links(url)\n",
    "                #print(\"DEBUG:\", url, scraped_pdf_links)\n",
    "                if len(scraped_pdf_links) > 0:\n",
    "                    pdf_link = scraped_pdf_links[0]\n",
    "            \n",
    "            if pdf_link != '':\n",
    "                # If link doesnt start with \"http\", we should replace \"abs\" in original link with \"pdf\"\n",
    "                # This happens on arxiv and ACM library\n",
    "                if not pdf_link.startswith(\"http\"):\n",
    "                    # arxiv and ACM library: replace \"abs/\" in original link with \"pdf/\"\n",
    "                    if \"abs/\" in url:\n",
    "                        pdf_link = url.replace(\"abs/\", \"pdf/\")\n",
    "                    # Springer links: replace \"/article/\" with the pdf_link\n",
    "                    if \"article/\" in url:\n",
    "                        index = url.find('/article/')\n",
    "                        pdf_link = url[:index] + pdf_link\n",
    "\n",
    "                pdf_links.append(pdf_link)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://link.springer.com/article/10.1023/a:1019956318069\n",
      "https://arxiv.org/abs/1803.02999\n",
      "http://proceedings.mlr.press/v97/finn19a.html?ref=https://githubhelp.com\n",
      "https://ieeexplore.ieee.org/abstract/document/9428530/\n",
      "https://www.sciencedirect.com/science/article/pii/S2352154621000024\n"
     ]
    }
   ],
   "source": [
    "pdf_links = google_scholar_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/1803.02999',\n",
       " 'http://proceedings.mlr.press/v97/finn19a/finn19a.pdf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(pdf_links):\n",
    "    directory = 'PDF Documents/searched/'\n",
    "    for i, link in enumerate(pdf_links):\n",
    "        if not link.endswith(\".pdf\"):\n",
    "            link += \".pdf\"\n",
    "        response = requests.get(link)\n",
    "        print(link)\n",
    "        print(response.status_code)\n",
    "        with open(os.path.join(directory, f'paper_{i}.pdf'), 'wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1803.02999.pdf\n",
      "200\n",
      "http://proceedings.mlr.press/v97/finn19a/finn19a.pdf\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "download_pdfs(pdf_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-agent set-up for CrewAI\n",
    "\n",
    "We can have multiple agents, each responsible for a particular area of research in AI.\n",
    "\n",
    "Each agent shares a tool - to search on Google Scholar for information, or to search their vector DB. Every time relevant Google Scholar papers appear, the agent will save those downloaded PDFs to their vector store, updating it in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing the agent/task/crew with simple math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_agent = Agent(role = \"Math Professor\",\n",
    "                      goal = \"\"\"Provide the solution to the students that are asking mathematical questions and give them the answer.\"\"\",\n",
    "                      backstory = \"\"\"You are an excellent math professor that likes to solve math questions in a way that everyone can understand your solution\"\"\",\n",
    "                      allow_delegation = False,\n",
    "                      verbose = True,\n",
    "                      llm = llm)\n",
    "task = Task (description=\"\"\"what is 3 + 5\"\"\",\n",
    "             expected_output=\"A math answer\",\n",
    "             agent = general_agent)\n",
    "\n",
    "crew = Crew(\n",
    "            agents=[general_agent],\n",
    "            tasks=[task],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "bedrock = create_client()\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id='amazon.titan-embed-text-v1', client=bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(self, pdf_links):\n",
    "    directory = 'PDF Documents/searched/'\n",
    "    for i, link in enumerate(pdf_links):\n",
    "        if not link.endswith(\".pdf\"):\n",
    "            link += \".pdf\"\n",
    "        response = requests.get(link)\n",
    "        with open(os.path.join(directory, f'paper_{i}.pdf'), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "def scrape_pdf_links(self, url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pdf_links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href')\n",
    "        content = a_tag.text.strip()\n",
    "\n",
    "        # If the link ends with \".pdf\" or \"PDF\" is in the text\n",
    "        if href:\n",
    "            pdf_in_children = any('PDF' in child.text.upper() for child in a_tag.find_all())\n",
    "\n",
    "            if href.endswith('.pdf') or 'PDF' in content.upper() or pdf_in_children:\n",
    "                pdf_links.append(href)\n",
    "    return pdf_links\n",
    "\n",
    "def google_scholar_search(self, query, top_n=5):\n",
    "    search_query = scholarly.search_pubs(query)\n",
    "    pdf_links = []\n",
    "    for i in range(top_n):\n",
    "        try:\n",
    "            result = next(search_query)\n",
    "            url = result['pub_url']\n",
    "            #print(url)\n",
    "\n",
    "            pdf_link = ''\n",
    "            if url.endswith(\".pdf\"):\n",
    "                pdf_link = url\n",
    "            else:\n",
    "                scraped_pdf_links = scrape_pdf_links(url)\n",
    "                #print(\"DEBUG:\", url, scraped_pdf_links)\n",
    "                if len(scraped_pdf_links) > 0:\n",
    "                    pdf_link = scraped_pdf_links[0]\n",
    "            \n",
    "            if pdf_link != '':\n",
    "                # If link doesnt start with \"http\", we should replace \"abs\" in original link with \"pdf\"\n",
    "                # This happens on arxiv and ACM library\n",
    "                if not pdf_link.startswith(\"http\"):\n",
    "                    # arxiv and ACM library: replace \"abs/\" in original link with \"pdf/\"\n",
    "                    if \"abs/\" in url:\n",
    "                        pdf_link = url.replace(\"abs/\", \"pdf/\")\n",
    "                    # Springer links: replace \"/article/\" with the pdf_link\n",
    "                    if \"article/\" in url:\n",
    "                        index = url.find('/article/')\n",
    "                        pdf_link = url[:index] + pdf_link\n",
    "\n",
    "                pdf_links.append(pdf_link)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    return pdf_links\n",
    "\n",
    "@tool(\"search-tool\")\n",
    "def search_concept_in_field(query: str):\n",
    "    \"\"\"Tool for an AI research agent to look up a concept related to its\n",
    "    field of specialty on Google Scholar, and process PDFs it finds\"\"\"\n",
    "    print(\"Performing Google Scholar search...\")\n",
    "    pdf_links = google_scholar_search(query)\n",
    "    download_pdfs(pdf_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = Agent(\n",
    "    role='Senior Research Analyst',\n",
    "    goal='Uncover cutting-edge developments in AI and data science',\n",
    "    backstory=\"\"\"\n",
    "    You work at a research firm that aims to stay at the forefront of AI technologies.\n",
    "    The firm aims to invent new technologies that apply the latest research. Your role\n",
    "    is to supply the latest and greatest findings in Generative AI.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    "    tool=[search_concept_in_field]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 21:15:53,296 - 8608358976 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "task1 = Task(\n",
    " description=\"Conduct a brief analysis of the latest GenerativeAI advancements in 2024, using the tool…\",\n",
    " expected_output=\"Full analysis report in bullet points\",\n",
    " agent=researcher\n",
    ")\n",
    "crew = Crew(\n",
    " agents=[researcher],\n",
    " tasks=[task1],\n",
    " verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Senior Research Analyst\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Conduct a brief analysis of the latest GenerativeAI advancements in 2024, using the tool…\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I'm excited to dive into the latest advancements in Generative AI!\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "**Generative AI Advancements in 2024 Analysis Report**\n",
      "\n",
      "• **Text-to-Image Generation**: Researchers from Meta AI and Google Brain introduced a new text-to-image generation model that can generate high-quality images given a textual description. The model uses a combination of transformers and convolutional neural networks to produce photorealistic images.\n",
      "\n",
      "• **Audio Synthesis**: A team from Stanford University and Google developed an audio synthesis model that can generate realistic audio clips based on a given prompt. The model uses a transformer-based architecture and can be applied to various applications such as music generation, voice assistants, and audio effects processing.\n",
      "\n",
      "• **Video Generation**: Researchers from the University of California, Berkeley, and Adobe demonstrated a video generation model that can create realistic videos based on a given text description or existing video frames. The model uses a combination of generative adversarial networks (GANs) and transformer-based architectures to produce high-quality videos.\n",
      "\n",
      "• **Language Modeling**: A team from Microsoft Research introduced a new language modeling architecture that can generate coherent and informative texts given a prompt. The model uses a combination of transformer-based and convolutional neural network-based architectures to produce high-quality text outputs.\n",
      "\n",
      "• **Data Augmentation**: Researchers from the University of California, Los Angeles (UCLA) developed a data augmentation framework that can generate realistic training data for AI models using generative models such as GANs and Variational Autoencoders (VAEs). The framework can be applied to various applications such as image classification, object detection, and natural language processing.\n",
      "\n",
      "• **Explainable AI**: A team from the University of Cambridge introduced an explainable AI framework that uses generative models to interpret and visualize the decision-making process of AI models. The framework can be applied to various applications such as medical diagnosis, financial forecasting, and autonomous driving.\n",
      "\n",
      "• **Human-AI Collaboration**: Researchers from Stanford University demonstrated a human-AI collaboration platform that integrates generative AI models with human feedback to generate high-quality creative content such as music, art, and writing. The platform can be applied to various applications such as entertainment, education, and healthcare.\n",
      "\n",
      "• **Generative Adversarial Networks (GANs)**: A team from Google Brain introduced a new GAN architecture that can generate realistic data samples that are indistinguishable from real-world data. The architecture uses a combination of transformers and convolutional neural networks to produce high-quality outputs.\n",
      "\n",
      "• **Variational Autoencoders (VAEs)**: Researchers from the University of California, Berkeley, developed a VAE-based generative model that can learn complex distributions in high-dimensional spaces. The model uses a combination of encoder-decoder architectures and Bayesian inference to produce high-quality outputs.\n",
      "\n",
      "• **Transformers**: A team from Meta AI introduced a new transformer architecture that can be used for various generative tasks such as text-to-image generation, audio synthesis, and language modeling. The architecture uses self-attention mechanisms and feed-forward neural networks to produce high-quality outputs.\n",
      "\n",
      "I hope this report meets the expectations!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Senior Research Analyst] Task output: **Generative AI Advancements in 2024 Analysis Report**\n",
      "\n",
      "• **Text-to-Image Generation**: Researchers from Meta AI and Google Brain introduced a new text-to-image generation model that can generate high-quality images given a textual description. The model uses a combination of transformers and convolutional neural networks to produce photorealistic images.\n",
      "\n",
      "• **Audio Synthesis**: A team from Stanford University and Google developed an audio synthesis model that can generate realistic audio clips based on a given prompt. The model uses a transformer-based architecture and can be applied to various applications such as music generation, voice assistants, and audio effects processing.\n",
      "\n",
      "• **Video Generation**: Researchers from the University of California, Berkeley, and Adobe demonstrated a video generation model that can create realistic videos based on a given text description or existing video frames. The model uses a combination of generative adversarial networks (GANs) and transformer-based architectures to produce high-quality videos.\n",
      "\n",
      "• **Language Modeling**: A team from Microsoft Research introduced a new language modeling architecture that can generate coherent and informative texts given a prompt. The model uses a combination of transformer-based and convolutional neural network-based architectures to produce high-quality text outputs.\n",
      "\n",
      "• **Data Augmentation**: Researchers from the University of California, Los Angeles (UCLA) developed a data augmentation framework that can generate realistic training data for AI models using generative models such as GANs and Variational Autoencoders (VAEs). The framework can be applied to various applications such as image classification, object detection, and natural language processing.\n",
      "\n",
      "• **Explainable AI**: A team from the University of Cambridge introduced an explainable AI framework that uses generative models to interpret and visualize the decision-making process of AI models. The framework can be applied to various applications such as medical diagnosis, financial forecasting, and autonomous driving.\n",
      "\n",
      "• **Human-AI Collaboration**: Researchers from Stanford University demonstrated a human-AI collaboration platform that integrates generative AI models with human feedback to generate high-quality creative content such as music, art, and writing. The platform can be applied to various applications such as entertainment, education, and healthcare.\n",
      "\n",
      "• **Generative Adversarial Networks (GANs)**: A team from Google Brain introduced a new GAN architecture that can generate realistic data samples that are indistinguishable from real-world data. The architecture uses a combination of transformers and convolutional neural networks to produce high-quality outputs.\n",
      "\n",
      "• **Variational Autoencoders (VAEs)**: Researchers from the University of California, Berkeley, developed a VAE-based generative model that can learn complex distributions in high-dimensional spaces. The model uses a combination of encoder-decoder architectures and Bayesian inference to produce high-quality outputs.\n",
      "\n",
      "• **Transformers**: A team from Meta AI introduced a new transformer architecture that can be used for various generative tasks such as text-to-image generation, audio synthesis, and language modeling. The architecture uses self-attention mechanisms and feed-forward neural networks to produce high-quality outputs.\n",
      "\n",
      "I hope this report meets the expectations!\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
